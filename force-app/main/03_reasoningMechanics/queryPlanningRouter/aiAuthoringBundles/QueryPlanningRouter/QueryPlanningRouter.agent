# QueryPlanningRouter - Context Classification + Technique-Based Routing
#
# Pattern: on every turn the LLM classifies the query domain, checks its
# previous answer for errors, picks a planning technique, and routes — all
# inside a single reasoning/instructions block. No external flows are needed
# for the planning stage; before_reasoning is only used in product_search
# where real retrieval systems (cache, SQL, knowledge) must be called before
# the LLM runs.
#
# Topic map:
#
#   start_agent query_planner  (hub — re-entered on every turn)
#     └── reasoning/instructions (single LLM pass)
#           Step 1 — check conversation history for errors in previous answer
#                    set @variables.previous_answer_has_error = ...
#           Step 2 — classify current question into a domain context tag
#                    set @variables.query_context = ...
#                    (payroll | benefit | talent | outsourcing | insurance |
#                     time_attendance | retirement | compliance | competitor |
#                     error | general)
#                    if previous answer had errors → override to "error"
#           Step 3 — pick a planning technique (if context != "error")
#                    Sub-query decomposition | HyDE | Step-back | Chain of thought
#           Step 4 — call the routing action
#                    escalate               if query_context == "error"
#                    route_to_product_search otherwise
#
#   topic product_search
#     ├── before_reasoning  →  qa_cache_lookup  (real retrieval — must run
#     │                        before LLM so cache result shapes instructions)
#     └── reasoning
#           cache hit  →  return cached answer immediately
#           cache miss →  generate_sql + knowledge_search, synthesise
#
#   topic escalation
#     Acknowledge the error, hand off to a human agent.
#
# Why no flows for query_planner?
#   classify_context, check_previous_answer, and store_response can all be
#   replaced by LLM reasoning:
#     • Classification   → set @variables.query_context = ...  (LLM infers)
#     • Error detection  → LLM reads its own last message from conversation
#                          history and evaluates quality
#     • Store response   → unnecessary; LLM always has conversation history
#   Use flows when you need deterministic/non-LLM logic (database lookup,
#   keyword rules, external API). Use reasoning for LLM-native tasks.
#
# Why before_reasoning in product_search?
#   qa_cache_lookup is a real database call that must complete BEFORE the LLM
#   decides whether to run SQL or knowledge search. That dependency requires
#   before_reasoning; it cannot be inlined into instructions.

config:
   agent_name: "QueryPlanningRouter"
   agent_label: "QueryPlanningRouter"
   agent_type: "AgentforceEmployeeAgent"
   description: "Classifies query context in reasoning, picks a planning technique, then routes to the right specialist topic"

variables:
   # Set by the LLM in reasoning/instructions on every turn.
   # Used by product_search instructions to tailor retrieval and the response.
   query_context: mutable string = ""
      description: "Domain context of the current user question, classified by the LLM each turn"

   # Set by the LLM in reasoning/instructions on every turn.
   # True when the LLM judges its own previous response to be erroneous.
   previous_answer_has_error: mutable boolean = False
      description: "True when the LLM detects an error in its previous response"

   # Set by qa_cache_lookup in product_search before_reasoning.
   cache_hit: mutable boolean = False
      description: "True when the Q&A cache contains an answer for the current question"

   cached_answer: mutable string = ""
      description: "The cached answer returned by qa_cache_lookup, empty on a cache miss"

system:
   messages:
      welcome: "Hi! I'm your product advisor. What would you like to know?"
      error: "Something went wrong. Let me connect you with a team member."

   instructions: >
      You are a knowledgeable HR and payroll product advisor.
      Always tailor your answers to the user's specific domain context.
      Be concise, accurate, and cite your reasoning when helpful.

# ── Entry point and turn hub ───────────────────────────────────────────────────
# Re-entered after every specialist topic completes.
# A single reasoning pass handles classification, error-checking, technique
# selection, and routing — no flows, no before_reasoning, no after_reasoning.
start_agent query_planner:
   description: "Classifies the query domain, plans how to answer it, then routes to the right specialist topic"

   reasoning:
      instructions:|
         Work through the following steps in order. Do not skip any step.

         ── Step 1: Check previous answer for errors ────────────────────────
         Look at the conversation history.
         If the agent has already responded at least once, evaluate whether
         that last response contained any of these problems:
           • Factual errors or contradictions
           • Explicit uncertainty ("I'm not sure", "I cannot confirm")
           • An incomplete or nonsensical answer
         Set the flag accordingly:
           set @variables.previous_answer_has_error = ...

         ── Step 2: Classify the current question ───────────────────────────
         If @variables.previous_answer_has_error is True, use "error".
         Otherwise classify the user's question into exactly one of:
           payroll         — pay runs, tax withholding, deductions, paychecks
           benefit         — health, dental, vision, FSA, HSA, leave
           talent          — recruiting, onboarding, performance, learning
           outsourcing     — managed services, BPO, PEO arrangements
           insurance       — workers comp, liability, group policies
           time_attendance — time tracking, scheduling, overtime, PTO
           retirement      — 401k, pension, retirement plans
           compliance      — legal, regulatory, audits, reporting
           competitor      — comparisons with other vendors or products
           error           — user reports a bug, wrong answer, or system issue
           general         — anything that does not fit the above
         Set the context:
           set @variables.query_context = ...

         ── Step 3: Pick a planning technique (skip if context is "error") ──
         Choose exactly one technique based on the question type:

           • Sub-query decomposition — the question has multiple independent
             parts each needing a separate lookup (e.g. "Compare payroll and
             benefit plan pricing and list the support SLAs for each").

           • HyDE (Hypothetical Document Embedding) — generate a short
             hypothetical answer, then use it to retrieve real matching
             documents. Best for open-ended questions where the user's
             phrasing may not match indexed documents.

           • Step-back prompting — the question is very specific; ask a
             broader version first to surface foundational knowledge (e.g.
             "Why did my paycheck change?" → "What factors affect paycheck
             calculations?").

           • Chain of thought — the answer requires sequential reasoning
             where each step depends on the previous one (eligibility
             calculations, compliance rule chains, error diagnosis).

         State the chosen technique and a 2-3 sentence plan.
         Do NOT attempt to answer the question — the routing action hands
         off to the specialist topic that does that.

         ── Step 4: Call the routing action ────────────────────────────────
         If @variables.query_context is "error" → call escalate.
         Otherwise → call route_to_product_search.

      actions:
         escalate: @utils.transition to @topic.escalation
            description: "Context is 'error' or previous answer had an error — escalate to a human agent"

         route_to_product_search: @utils.transition to @topic.product_search
            description: "Route payroll, benefit, talent, compliance, and all other domain questions to the product search topic"

# ── Specialist: Product Search ─────────────────────────────────────────────────
# Handles all non-error domain questions.
# before_reasoning is used here because qa_cache_lookup is a real database
# call that must complete before the LLM runs so its result can gate the
# SQL and knowledge-search actions via 'available when'.
topic product_search:
   description: "Answers domain questions using Q&A cache lookup, SQL generation, and knowledge search"

   actions:
      qa_cache_lookup:
         description: "Look up the current question in the Q&A response cache"
         inputs:
            query_context: string
               description: "The domain context of the question"
            user_input: string
               description: "The user's question"
         outputs:
            cache_hit: boolean
               description: "True if a cached answer was found"
            cached_answer: string
               description: "The cached answer; empty if cache_hit is False"
         target: "flow://QACacheLookup"

      generate_sql:
         description: "Generate and run a SQL query for structured product or plan data"
         inputs:
            query_context: string
               description: "The domain context (e.g. payroll, benefit)"
            user_question: string
               description: "The natural-language question to answer"
         outputs:
            sql_result: string
               description: "Query result formatted as a readable summary"
            sql_success: boolean
               description: "True if the query ran without errors"
         target: "flow://GenerateAndRunSQL"

      knowledge_search:
         description: "Search the knowledge base for articles relevant to the question"
         inputs:
            query_context: string
               description: "The domain context to scope the search"
            user_question: string
               description: "The natural-language question"
         outputs:
            knowledge_result: string
               description: "Top-K relevant passages from the knowledge base"
            results_found: boolean
               description: "True if at least one relevant passage was found"
         target: "flow://KnowledgeSearch"

   reasoning:
      # qa_cache_lookup must run before the LLM so that cache_hit is known
      # when 'available when not @variables.cache_hit' is evaluated.
      before_reasoning:
         run @actions.qa_cache_lookup
            with query_context=@variables.query_context
            with user_input=@system_variables.user_input
            set @variables.cache_hit = @outputs.cache_hit
            set @variables.cached_answer = @outputs.cached_answer

      instructions:->
         | Domain: {!@variables.query_context}

         if @variables.cache_hit:
            | A cached answer was found for this question:

              {!@variables.cached_answer}

              Return that answer to the user. Do not call generate_sql or
              knowledge_search — the cached answer is sufficient.
         else:
            | No cached answer found. Retrieve and synthesise a fresh answer:
              1. Call generate_sql to fetch structured product/plan data.
              2. Call knowledge_search to retrieve relevant knowledge articles.
              3. Synthesise both results into a clear, accurate answer
                 tailored to the domain: {!@variables.query_context}.

      actions:
         run_sql: @actions.generate_sql
            with query_context=@variables.query_context
            with user_question=@system_variables.user_input
            available when not @variables.cache_hit

         search_knowledge: @actions.knowledge_search
            with query_context=@variables.query_context
            with user_question=@system_variables.user_input
            available when not @variables.cache_hit

         done: @utils.transition to @topic.query_planner
            description: "Answer delivered — return to the query planner for the next question"

# ── Specialist: Escalation ─────────────────────────────────────────────────────
topic escalation:
   description: "Handles error context — acknowledges the issue and transfers to a human agent"

   reasoning:
      instructions:|
         The query context is 'error'.
         This means one of two things:
           • The user's question itself signals an error, system issue, or
             a complaint that requires human intervention.
           • The LLM judged its own previous response as incorrect or
             incomplete and set previous_answer_has_error to True.

         Respond with empathy. Briefly acknowledge the issue.
         Do NOT attempt to re-answer the failed question.
         Explain that you are transferring the conversation to a human agent
         who can resolve it correctly.

         Once you have delivered the message, use return_to_planner so the
         user can continue with new questions after the transfer.

      actions:
         return_to_planner: @utils.transition to @topic.query_planner
            description: "Transfer acknowledged — return to query planner for any follow-up questions"
